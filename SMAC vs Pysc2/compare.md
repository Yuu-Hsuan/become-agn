# 以[SMAC](https://github.com/oxwhirl/smac/tree/master)及[Pysc2](https://github.com/nicoladainese96/SC2-RL)做比較

### 目標 : 讓 PySC2 變得像 SMAC 一樣好用和專業，但只針對「單代理人」的設計
不考慮SMAC的多代理人，專注於讓Pysc2的單代理人能夠完成有挑戰的任務。因此需要補充一些功能，讓 PySC2 不僅靈活，還能更容易使用、更適合訓練 AI

## 初步比較
|特性|SMAC|PySC2|
| --- | --- | --- |
|概述|專門為訓練「多代理人合作」設計的工具，目的是讓程式控制多個角色在星海爭霸中互相配合，例如團隊作戰或協同完成挑戰任務|是個工具，用電腦程式控制《SC II》中的單位，執行各種行為。相當靈活，但需要使用者自行設計任務和細節|
|目標用途|多代理人強化學習（MARL）專用環境|通用的 SC II 強化學習工具|
|簡單易用性|提供高層次 API，專注於 MARL 場景|提供較低層次的 API，用戶需自行構建場景|
|環境穩定性|高度優化，支持多種 RL 框架|API 相對簡單，可能需大量調適|
|內建場景|預定義多智能體挑戰場景|僅提供基礎地圖和交互，需自行設計場景|
|場景細緻度|精心設計、平衡的地圖和目標|無場景預設，難度控制依賴使用者|
|社群支持|成熟的 MARL 社群支持，研究文獻豐富|用於多種 RL 研究，缺乏相關社群|

## 要使 PySC2 達到像 SMAC 那樣的成熟程度，預估要做以下幾件事：

### 1. 新增任務框架： 設計單智能體的標準化任務和目標

|SMAC|PySC2|
| --- | --- |
|已設計好場景，使用者只需要訓練 AI 去完成目標|沒設定任何目標或規則。需再告訴代理人：「去收集10個礦」、「消滅地圖上的敵人」|


* 加入任務框架：讓代理人學會

|任務|描述|
| --- | --- |
|收集資源任務|角色需要找到地圖上的礦，並帶回基地|
|生存挑戰|角色需要在敵人的攻擊下存活更久|
|對戰任務|角色需要打敗某個敵人|


* 加上 reward 值：

在遊戲中，用reward 值告訴代理人「做得好」或「做得不好」，這樣代理人能更清楚哪些行為是好的是壞的


### 2. 高層次操作封裝： 提供簡化的行為與觀察接口，讓電腦更容易控制角色
|描述|SMAC|PySC2|
| --- | --- | --- |
|操作|高層次操作|低層次操作|
|動作搭配程式指令|將角色的基本操作進行了封裝，用戶不需要處理所有細節，而是直接控制更簡單的行為:<br>1. 移動：角色移動到目標位置，內部自動處理路徑規劃<br>2. 攻擊：選擇目標後，角色自動執行攻擊動作，無需指定技能類型或攻擊距離|1. 選擇一個單位:角色的位置和目標的位置<br>2. 設定單位移動到某個座標 (x, y):執行的技能類型（如普通攻擊或特殊技能）<br>3. 使用技能或攻擊某個目標:移動過程中碰到障礙的處理|
|AI訓練難度|操作過於繁瑣，特別是在路徑規劃或行為組合時，我們需要投入額外的精力處理基礎行動|只能專注於決策層面的學習（如「何時移動」「何時攻擊」），而不是學習基礎操作|


### 3. 優化環境穩定性：模擬一個合理的遊戲世界
|SMAC|PySC2|
| --- | --- |
|目前的環境是靜態的，敵人不會主動攻擊或做出其他動作|敵人會有一些規則行為|

1. 加入簡單的敵人行為規則
   * 敵人主動移動並攻擊角色，比如「看到角色後追蹤並攻擊」
   * 敵人巡邏地圖上的特定區域
   
2. 設定更靈活的場景初始化：讓 AI 適應不同的挑戰
   如：
   * 讓我們決定角色的位置
   * 我們配置地圖上的資源分佈或敵人的數量

### 4. 基準測試設計： 提供標準場景與預設算法基準結果，以偵測 AI 表現如何

|SMAC|PySC2|
| --- | --- |
|已設計好標準場景和基準結果（如一個好的 AI 在場景中能達到 80 分）| 缺少標準，導致訓練的 AI 沒有對比依據，不知做得好不好|

1. 設計幾個標準的測試場景，如：
   * 資源收集任務：角色需要收集 50 單位的資源
   * 擊敗敵人任務：角色要在 1v1 的情況下打敗敵人
   * 困難任務：資源和敵人數量更複雜，挑戰 AI 的策略能力
     
2. 提供這些場景的預設「基準分數」，如：
   * 簡單任務的基準得分是 100 分，若 AI 訓練後能超過基準，就代表它較強



